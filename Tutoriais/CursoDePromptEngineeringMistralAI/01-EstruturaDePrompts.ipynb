{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPYYbU82YJFpYflpY5GXVyC"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["Instalação da biblioteca oficial da Mistral"],"metadata":{"id":"r8ZhvJSaFMlt"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"1C61ZpSUFGhD"},"outputs":[],"source":["!pip install mistralai"]},{"cell_type":"markdown","source":["Configurando a API"],"metadata":{"id":"R8yQyvUQFmTk"}},{"cell_type":"code","source":["import os\n","from mistralai import Mistral\n","\n","# Pegue sua chave em https://mistral.ai/ e cole aqui, ou defina via variável de ambiente\n","api_key = os.getenv(\"MISTRAL_API_KEY\", \"sua_chave_de_api_aqui\")\n","\n","client = Mistral(api_key=api_key)\n","MODEL_NAME = \"mistral-small-latest\"  # Ou 'mistral-medium-latest', 'mistral-large-latest'"],"metadata":{"id":"zCLYnx1VFuNT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Criando uma função para gerar respostas com controle de tokens"],"metadata":{"id":"vob04KaOFvDl"}},{"cell_type":"code","source":["def gerar_resposta(prompt, max_tokens=300, temperature=0.0):\n","    response = client.chat.complete(\n","        model=MODEL_NAME,\n","        messages=[{\"role\": \"user\", \"content\": prompt}],\n","        max_tokens=max_tokens,\n","        temperature=temperature\n","    )\n","    content = response.choices[0].message.content\n","    print(f\"Tokens usados: {response.usage.total_tokens}\")\n","    return content"],"metadata":{"id":"Rsr9E6dsF9e4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Exemplo 1 - Prompt para se evitar (muito vago):"],"metadata":{"id":"zdWn8VAyF_NF"}},{"cell_type":"code","source":["prompt = \"Explique IA.\"\n","print(gerar_resposta(prompt, max_tokens=200))"],"metadata":{"id":"Io1KtayaGgzU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Exemplo 2 - Prompt Claro e Estruturado (Recomendado):"],"metadata":{"id":"XuEixak2Gid9"}},{"cell_type":"code","source":["prompt = (\n","    \"Você é um especialista em Inteligência Artificial. \"\n","    \"Explique de forma objetiva o que é Inteligência Artificial, \"\n","    \"usando exemplos práticos e limitando a resposta a 5 linhas.\\n\\n\"\n","    \"Formato da resposta:\\n\"\n","    \"Definição: ...\\n\"\n","    \"Exemplos: ...\"\n",")\n","print(gerar_resposta(prompt, max_tokens=200))"],"metadata":{"id":"UXd5lgm3GoCi"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Testando os limites da API"],"metadata":{"id":"AEADHnpyGrQu"}},{"cell_type":"code","source":["import time\n","\n","prompts = [\n","    \"Defina aprendizado de máquina.\",\n","    \"Explique o conceito de overfitting.\",\n","    \"O que é NLP?\",\n","    \"Explique o funcionamento de LLMs.\"\n","]\n","\n","for i, prompt in enumerate(prompts):\n","    print(f\"Requisição {i+1}:\")\n","    try:\n","        print(gerar_resposta(prompt, max_tokens=150))\n","    except Exception as e:\n","        print(f\"Erro ou limite atingido: {e}\")\n","    time.sleep(0.5)  # Diminua para testar o limite de RPS"],"metadata":{"id":"iIfIZMiNGt-D"},"execution_count":null,"outputs":[]}]}