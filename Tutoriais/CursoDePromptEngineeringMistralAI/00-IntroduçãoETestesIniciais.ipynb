{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOOQ5kuyLMBJFL+5Eat9j60"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["Instalação da biblioteca oficial da Mistral"],"metadata":{"id":"UYBnDxb3D-ep"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"_k6nQKyO-Mbu"},"outputs":[],"source":["!pip install mistralai"]},{"cell_type":"markdown","source":["Configurando a API"],"metadata":{"id":"1sd2KMdWEFGM"}},{"cell_type":"code","source":["import os\n","from mistralai import Mistral\n","\n","# Pegue sua chave em https://mistral.ai/ e cole aqui, ou defina via variável de ambiente\n","api_key = os.getenv(\"MISTRAL_API_KEY\", \"sua_chave_de_api_aqui\")\n","\n","client = Mistral(api_key=api_key)\n","MODEL_NAME = \"mistral-small-latest\"  # Ou 'mistral-medium-latest', 'mistral-large-latest'\n"],"metadata":{"id":"lwI9OXOKEHrw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Criando a função para enviar prompt com controle de tokens"],"metadata":{"id":"0xGeMSMyEJ7X"}},{"cell_type":"code","source":["def gerar_resposta(prompt, max_tokens=300, temperature=0.0):\n","    response = client.chat.complete(\n","        model=MODEL_NAME,\n","        messages=[{\"role\": \"user\", \"content\": prompt}],\n","        max_tokens=max_tokens,\n","        temperature=temperature\n","    )\n","    print(f\"Tokens usados: {response.usage.total_tokens}\")\n","    return response.choices[0].message.content"],"metadata":{"id":"tzn8QJRfEJji"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Testando o prompt"],"metadata":{"id":"xfsgVy_OEQy3"}},{"cell_type":"code","source":["print(gerar_resposta(\"Explique o que é engenharia de prompts.\", max_tokens=200))"],"metadata":{"id":"UiUmbn-MES98"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Teste de limites da API (limite de 1 requisição por segundo)"],"metadata":{"id":"3GL0zwTCEVVD"}},{"cell_type":"code","source":["import time\n","\n","prompts = [\n","    \"O que é inteligência artificial?\",\n","    \"Explique o conceito de LLM.\",\n","    \"O que é Mistral AI?\",\n","    \"Como funciona um modelo transformer?\"\n","]\n","\n","for i, prompt in enumerate(prompts):\n","    print(f\"Requisição {i+1}\")\n","    try:\n","        print(gerar_resposta(prompt, max_tokens=150))\n","    except Exception as e:\n","        print(f\"Erro ou limite atingido: {e}\")\n","    time.sleep(0.5)  # Reduza esse tempo para testar limites de RPS"],"metadata":{"id":"O3R_CyV0EfKk"},"execution_count":null,"outputs":[]}]}